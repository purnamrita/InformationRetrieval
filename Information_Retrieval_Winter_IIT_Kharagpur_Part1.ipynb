{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUZRw1YQMYXB",
        "outputId": "b90929dd-538a-4ee4-f08c-c39f6a3a8312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Why-_lzONKjh"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    return word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uOgidCvOHw1",
        "outputId": "063a6ea2-b4fd-4c3f-d3a3-b532294853cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', '.', 'Welcome', 'to', 'GeeksforGeeks', '.']\n"
          ]
        }
      ],
      "source": [
        "print(tokenize(\"Hello everyone. Welcome to GeeksforGeeks.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adgw6j-POlxN",
        "outputId": "f0a73632-0b89-4643-e52c-53f99b9b139f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "stop_words_list = stopwords.words('english')\n",
        "print(stop_words_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5_o5EdGO3SY"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        "    list_of_tokens = tokenize(text)\n",
        "    tokens_without_stopwords = []\n",
        "    for word in list_of_tokens:\n",
        "        if word.lower() not in stop_words_list and len(word) >= 2:   # punctuations are also removed\n",
        "            tokens_without_stopwords.append(word)\n",
        "\n",
        "    return tokens_without_stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNJbfxrEPl_k",
        "outputId": "09b545ed-9a75-4d4c-97e6-b8d544457be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', 'Welcome', 'GeeksforGeeks']\n"
          ]
        }
      ],
      "source": [
        "print(remove_stopwords(\"Hello everyone. Welcome to GeeksforGeeks.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKbU08QMQGjf"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatization(text):\n",
        "    lemmatized_words = []\n",
        "    tokens = remove_stopwords(text)\n",
        "    for word in tokens:\n",
        "        lemmatized_words.append(lemmatizer.lemmatize(word))\n",
        "    return lemmatized_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_iVpUHWRPri",
        "outputId": "0206ea2f-0d0c-4bb3-ab43-ec9bf226e163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', 'Welcome', 'GeeksforGeeks']\n"
          ]
        }
      ],
      "source": [
        "print(lemmatization(\"Hello everyone. Welcome to GeeksforGeeks.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHj3Y6uVRnOb",
        "outputId": "969d68c4-de71-4daa-fab2-972d513e03c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['rock', 'corpus', 'going']\n"
          ]
        }
      ],
      "source": [
        "print(lemmatization(\"rocks corpora won going has\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOAoaldQYRKR",
        "outputId": "31e78ec9-b5ec-4ddf-ac97-de9e0b0aa1a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['new', 'home', 'sale', 'top', 'forecast']\n",
            "['home', 'sale', 'rise', 'july']\n",
            "['increase', 'home', 'sale', 'july']\n",
            "['july', 'new', 'home', 'sale', 'rise']\n",
            "[['new', 'home', 'sale', 'top', 'forecast'], ['home', 'sale', 'rise', 'july'], ['increase', 'home', 'sale', 'july'], ['july', 'new', 'home', 'sale', 'rise']]\n",
            "['new', 'home', 'sale', 'top', 'forecast', 'home', 'sale', 'rise', 'july', 'increase', 'home', 'sale', 'july', 'july', 'new', 'home', 'sale', 'rise']\n"
          ]
        }
      ],
      "source": [
        "\"\"\"doc1 = \"I new home sales top forecasts\"\n",
        "doc2 = \"home sales rise in july\"\n",
        "doc3 = \"increase in home sales in july\"\n",
        "doc4 = \"july new home sales rise\" \"\"\"\n",
        "\n",
        "doc1 = open(\"doc1.txt\").read()\n",
        "doc2 = open(\"doc2.txt\").read()\n",
        "doc3 = open(\"doc3.txt\").read()\n",
        "doc4 = open(\"doc4.txt\").read()\n",
        "\n",
        "doc1 = lemmatization(doc1)\n",
        "doc2 = lemmatization(doc2)\n",
        "doc3 = lemmatization(doc3)\n",
        "doc4 = lemmatization(doc4)\n",
        "docs = [doc1, doc2, doc3, doc4]\n",
        "print(doc1)\n",
        "print(doc2)\n",
        "print(doc3)\n",
        "print(doc4)\n",
        "print(docs)\n",
        "\n",
        "list_of_available_words_for_search = doc1 + doc2 + doc3 + doc4\n",
        "print(list_of_available_words_for_search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ2UdnFtYKSM",
        "outputId": "1cbff8fc-1096-465d-bee7-8fc0828af9d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'new': {0, 3},\n",
              " 'home': {0, 1, 2, 3},\n",
              " 'sale': {0, 1, 2, 3},\n",
              " 'top': {0},\n",
              " 'forecast': {0},\n",
              " 'rise': {1, 3},\n",
              " 'july': {1, 2, 3},\n",
              " 'increase': {2}}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "inverted_index = {}\n",
        "\n",
        "for i, doc in enumerate(docs):\n",
        "    for term in doc:\n",
        "        if term in inverted_index:\n",
        "            inverted_index[term].add(i)\n",
        "        else: inverted_index[term] = {i}\n",
        "\n",
        "inverted_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q5NkSprbcym",
        "outputId": "3e7979f3-08e9-4211-b159-32b2ecdf7514"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 3}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "inverted_index['new']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJQiexHnbi7S",
        "outputId": "cb9374fa-9133-4c67-bdc6-701ac5716011"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "inverted_index['sale']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSGh_s1TYXW0"
      },
      "outputs": [],
      "source": [
        "def or_postings(posting1, posting2):\n",
        "    p1 = 0\n",
        "    p2 = 0\n",
        "    result = list()\n",
        "    while p1 < len(posting1) and p2 < len(posting2):\n",
        "        if posting1[p1] == posting2[p2]:\n",
        "            result.append(posting1[p1])\n",
        "            p1 += 1\n",
        "            p2 += 1\n",
        "        elif posting1[p1] > posting2[p2]:\n",
        "            result.append(posting2[p2])\n",
        "            p2 += 1\n",
        "        else:\n",
        "            result.append(posting1[p1])\n",
        "            p1 += 1\n",
        "    while p1 < len(posting1):\n",
        "        result.append(posting1[p1])\n",
        "        p1 += 1\n",
        "    while p2 < len(posting2):\n",
        "        result.append(posting2[p2])\n",
        "        p2 += 1\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQbd-m20brg9"
      },
      "outputs": [],
      "source": [
        "posting1 = list(inverted_index[\"top\"])\n",
        "posting2 = list(inverted_index[\"july\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pAVTngecK5D",
        "outputId": "bf4cb0e2-6872-489b-a245-e7c2b89565c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3]\n"
          ]
        }
      ],
      "source": [
        "print(or_postings(posting1, posting2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlrNoPrnYa_v"
      },
      "outputs": [],
      "source": [
        "def and_postings(posting1, posting2):\n",
        "    p1 = 0\n",
        "    p2 = 0\n",
        "    result = list()\n",
        "    while p1 < len(posting1) and p2 < len(posting2):\n",
        "        if posting1[p1] == posting2[p2]:\n",
        "            result.append(posting1[p1])\n",
        "            p1 += 1\n",
        "            p2 += 1\n",
        "        elif posting1[p1] > posting2[p2]:\n",
        "            p2 += 1\n",
        "        else:\n",
        "            p1 += 1\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flNmfDe4c-ba",
        "outputId": "913b5fc8-4bef-4a67-fc4d-7cd9a6676cbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "print(and_postings(posting1, posting2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye729VyBp4OZ",
        "outputId": "3ff1e95d-6cfe-4c48-fc47-a8dabf224731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter word to be searched : hi hello\n",
            "Not available\n"
          ]
        }
      ],
      "source": [
        "a = input(\"Enter word to be searched : \")\n",
        "if a not in list_of_available_words_for_search:\n",
        "    print(\"Not available\")\n",
        "else:\n",
        "    print(inverted_index[a])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DN8GQ32pZZ1",
        "outputId": "a6355818-3f70-4fd3-8718-734f88869a1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter search query: july forecasts\n",
            "['july', 'forecasts']\n",
            "['july', 'forecasts']\n",
            "['july', 'forecast']\n"
          ]
        }
      ],
      "source": [
        "query = input(\"Enter search query: \")\n",
        "words = query.split()\n",
        "res = []\n",
        "print(words)\n",
        "for word in words:\n",
        "    res += lemmatization(word)\n",
        "print(words)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpdXq3Z6pmmS",
        "outputId": "b34d9854-d092-4000-aa1d-67a8a1185eda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3]\n"
          ]
        }
      ],
      "source": [
        "set_of_docs = []\n",
        "for word in res:\n",
        "    try:\n",
        "        new_docs = inverted_index[word]\n",
        "    except KeyError:\n",
        "        continue\n",
        "    for _ in new_docs:\n",
        "        if _ not in set_of_docs:\n",
        "            set_of_docs.append(_)\n",
        "set_of_docs.sort()\n",
        "print(set_of_docs)\n",
        "if len(set_of_docs) == 0:\n",
        "    print(\"Not available\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bag of words\n",
        "bow = {}\n",
        "doc = open(\"doc.txt\").read()\n",
        "words = doc.split()\n",
        "print(words)\n",
        "for word in words:\n",
        "    if word in bow:\n",
        "        bow[word] += 1\n",
        "    else:\n",
        "        bow[word] = 1\n",
        "print(bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CHq3pF3aQgc",
        "outputId": "2fd95c2f-6983-4d6c-d3d9-4b0020f086a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Dursleys', 'are', 'a', 'well-to-do,', 'status-conscious', 'family', 'living', 'in', 'Surrey,', 'England.', 'Eager', 'to', 'keep', 'up', 'proper', 'appearances,', 'they', 'are', 'embarrassed', 'by', 'Mrs.', 'Dursley’s', 'eccentric', 'sister,', 'Mrs.', 'Potter,', 'whom', 'for', 'years', 'Mrs.', 'Dursley', 'has', 'pretended', 'not', 'to', 'know.', 'On', 'his', 'way', 'to', 'work', 'one', 'ordinary', 'morning,', 'Mr.', 'Dursley', 'notices', 'a', 'cat', 'reading', 'a', 'map.', 'He', 'is', 'unsettled,', 'but', 'tells', 'himself', 'that', 'he', 'has', 'only', 'imagined', 'it.', 'Then,', 'as', 'Mr.', 'Dursley', 'is', 'waiting', 'in', 'traffic,', 'he', 'notices', 'people', 'dressed', 'in', 'brightly', 'colored', 'cloaks.', 'Walking', 'past', 'a', 'bakery', 'later', 'that', 'day,', 'he', 'overhears', 'people', 'talking', 'in', 'an', 'excited', 'manner', 'about', 'his', 'sister-in-law’s', 'family,', 'the', 'Potters,', 'and', 'the', 'Potters’', 'one-year-old', 'son,', 'Harry.', 'Disturbed', 'but', 'still', 'not', 'sure', 'anything', 'is', 'wrong,', 'Mr.', 'Dursley', 'decides', 'not', 'to', 'say', 'anything', 'to', 'his', 'wife.', 'On', 'the', 'way', 'home,', 'he', 'bumps', 'into', 'a', 'strangely', 'dressed', 'man', 'who', 'gleefully', 'exclaims', 'that', 'someone', 'named', '“You-Know-Who”', 'has', 'finally', 'gone', 'and', 'that', 'even', 'a', '“Muggle”', 'like', 'Mr.', 'Dursley', 'should', 'rejoice.', 'Meanwhile,', 'the', 'news', 'is', 'full', 'of', 'unusual', 'reports', 'of', 'shooting', 'stars', 'and', 'owls', 'flying', 'during', 'the', 'day.']\n",
            "{'The': 1, 'Dursleys': 1, 'are': 2, 'a': 6, 'well-to-do,': 1, 'status-conscious': 1, 'family': 1, 'living': 1, 'in': 4, 'Surrey,': 1, 'England.': 1, 'Eager': 1, 'to': 5, 'keep': 1, 'up': 1, 'proper': 1, 'appearances,': 1, 'they': 1, 'embarrassed': 1, 'by': 1, 'Mrs.': 3, 'Dursley’s': 1, 'eccentric': 1, 'sister,': 1, 'Potter,': 1, 'whom': 1, 'for': 1, 'years': 1, 'Dursley': 5, 'has': 3, 'pretended': 1, 'not': 3, 'know.': 1, 'On': 2, 'his': 3, 'way': 2, 'work': 1, 'one': 1, 'ordinary': 1, 'morning,': 1, 'Mr.': 4, 'notices': 2, 'cat': 1, 'reading': 1, 'map.': 1, 'He': 1, 'is': 4, 'unsettled,': 1, 'but': 2, 'tells': 1, 'himself': 1, 'that': 4, 'he': 4, 'only': 1, 'imagined': 1, 'it.': 1, 'Then,': 1, 'as': 1, 'waiting': 1, 'traffic,': 1, 'people': 2, 'dressed': 2, 'brightly': 1, 'colored': 1, 'cloaks.': 1, 'Walking': 1, 'past': 1, 'bakery': 1, 'later': 1, 'day,': 1, 'overhears': 1, 'talking': 1, 'an': 1, 'excited': 1, 'manner': 1, 'about': 1, 'sister-in-law’s': 1, 'family,': 1, 'the': 5, 'Potters,': 1, 'and': 3, 'Potters’': 1, 'one-year-old': 1, 'son,': 1, 'Harry.': 1, 'Disturbed': 1, 'still': 1, 'sure': 1, 'anything': 2, 'wrong,': 1, 'decides': 1, 'say': 1, 'wife.': 1, 'home,': 1, 'bumps': 1, 'into': 1, 'strangely': 1, 'man': 1, 'who': 1, 'gleefully': 1, 'exclaims': 1, 'someone': 1, 'named': 1, '“You-Know-Who”': 1, 'finally': 1, 'gone': 1, 'even': 1, '“Muggle”': 1, 'like': 1, 'should': 1, 'rejoice.': 1, 'Meanwhile,': 1, 'news': 1, 'full': 1, 'of': 2, 'unusual': 1, 'reports': 1, 'shooting': 1, 'stars': 1, 'owls': 1, 'flying': 1, 'during': 1, 'day.': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = open(\"doc1.txt\").read()\n",
        "doc2 = open(\"doc2.txt\").read()\n",
        "doc3 = open(\"doc3.txt\").read()\n",
        "doc4 = open(\"doc4.txt\").read()\n",
        "\n",
        "doc1 = lemmatization(doc1)\n",
        "doc2 = lemmatization(doc2)\n",
        "doc3 = lemmatization(doc3)\n",
        "doc4 = lemmatization(doc4)\n",
        "docs = [doc1, doc2, doc3, doc4]\n",
        "\n",
        "print(doc1)\n",
        "print(doc2)\n",
        "print(doc3)\n",
        "print(doc4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLZPOI4yc7dr",
        "outputId": "496f7d30-9a88-4a25-f282-f22e30d8a8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['new', 'home', 'sale', 'top', 'forecast']\n",
            "['home', 'sale', 'rise', 'july']\n",
            "['increase', 'home', 'sale', 'july']\n",
            "['july', 'new', 'home', 'sale', 'rise']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_words = []\n",
        "for doc in docs:\n",
        "    for word in doc:\n",
        "        if word not in unique_words:\n",
        "            unique_words.append(word)\n",
        "\n",
        "print(unique_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY0WzbyVehQ1",
        "outputId": "bcbc5cbc-7f37-44a9-e4c8-3d4a91a90a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['new', 'home', 'sale', 'top', 'forecast', 'rise', 'july', 'increase']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF term frequency\n",
        "# frequency of a term in a given document / no of documents\n",
        "# count\n",
        "no_of_documents = len(docs)\n",
        "\n",
        "for doc in docs:\n",
        "    v = []\n",
        "    for word in unique_words:\n",
        "        v.append(doc.count(word))\n",
        "    print(v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc2joTZifdLd",
        "outputId": "2c978a6b-f5a6-4ffb-a662-fe15c6482d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 0, 0, 0]\n",
            "[0, 1, 1, 0, 0, 1, 1, 0]\n",
            "[0, 1, 1, 0, 0, 0, 1, 1]\n",
            "[1, 1, 1, 0, 0, 1, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF term frequency\n",
        "# frequency of a term in a given document / no of documents\n",
        "# actual TF\n",
        "no_of_documents = len(docs)\n",
        "\n",
        "for doc in docs:\n",
        "    v = []\n",
        "    for word in unique_words:\n",
        "        v.append(doc.count(word) / no_of_documents)\n",
        "    print(v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83AWI3aZlzIA",
        "outputId": "1e9009d8-4ea3-4051-a5b0-c97ad94844cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.25, 0.25, 0.25, 0.25, 0.25, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.25, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0]\n",
            "[0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.25, 0.25]\n",
            "[0.25, 0.25, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF term frequency\n",
        "# frequency of a term in a given document / no of documents\n",
        "# actual TF\n",
        "\n",
        "tf = []\n",
        "no_of_documents = len(docs)\n",
        "\n",
        "for doc in docs:\n",
        "    v = []\n",
        "    for word in unique_words:\n",
        "        v.append(doc.count(word) / no_of_documents)\n",
        "    tf.append(v)\n",
        "    print(v)\n",
        "print(tf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqH9-aLMmElN",
        "outputId": "67bef8fc-9946-4ad2-9ab3-7c7c1d606b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.25, 0.25, 0.25, 0.25, 0.25, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.25, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0]\n",
            "[0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.25, 0.25]\n",
            "[0.25, 0.25, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0]\n",
            "[[0.25, 0.25, 0.25, 0.25, 0.25, 0.0, 0.0, 0.0], [0.0, 0.25, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0], [0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.25, 0.25], [0.25, 0.25, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IDF of the unique words\n",
        "import math\n",
        "\n",
        "idf = []\n",
        "for word in unique_words:\n",
        "    count = 0\n",
        "    for doc in docs:\n",
        "        if word in doc:\n",
        "            count += 1\n",
        "    idf.append(math.log(len(docs) / count, 10))\n",
        "print(idf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpvLzJEFnbYF",
        "outputId": "18405463-d75c-4916-a40d-a75f53ef7e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.30102999566398114, 0.0, 0.0, 0.6020599913279623, 0.6020599913279623, 0.30102999566398114, 0.1249387366082999, 0.6020599913279623]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf-idf\n",
        "tf_idf = []\n",
        "for row in tf:\n",
        "    new_row = []\n",
        "    for i in range(len(row)):\n",
        "        new_row.append(row[i] * idf[i])\n",
        "    tf_idf.append(new_row)\n",
        "print(tf_idf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbpzVuAwocj7",
        "outputId": "ce8891f2-fc2a-451d-9c0e-a01b782bd88d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.07525749891599529, 0.0, 0.0, 0.15051499783199057, 0.15051499783199057, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.07525749891599529, 0.031234684152074976, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.031234684152074976, 0.15051499783199057], [0.07525749891599529, 0.0, 0.0, 0.0, 0.0, 0.07525749891599529, 0.031234684152074976, 0.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the tf-idf for the query\n",
        "# Find it's cosine similarity with the documents\n",
        "# Rank the results according to cosine similarity\n",
        "\n",
        "query = input(\"Enter search query: \")\n",
        "words = query.split()\n",
        "res = []\n",
        "print(words)\n",
        "for word in words:\n",
        "    res += lemmatization(word)\n",
        "print(words)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bBzZ4K1pntA",
        "outputId": "1af6da49-4a83-46be-d845-a65553cdbd50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter search query: july forecasts\n",
            "['july', 'forecasts']\n",
            "['july', 'forecasts']\n",
            "['july', 'forecast']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the tf-idf for the query\n",
        "# res contains the list of lemmatized words of the query\n",
        "# Each unique word appears in the query how many times\n",
        "\n",
        "tf_query = []\n",
        "\n",
        "for word in unique_words:\n",
        "    v.append(doc.count(word) / no_of_documents)\n",
        "tf.append(v)\n",
        "print(v)\n",
        "print(tf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zPC14VnJGwO",
        "outputId": "a12e5b17-2348-40e4-b55f-d45b39f2395c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.25, 0.25, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0, 0.25, 0.25, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0]\n",
            "[[0.25, 0.25, 0.25, 0.25, 0.25, 0.0, 0.0, 0.0], [0.0, 0.25, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0], [0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.25, 0.25], [0.25, 0.25, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0, 0.25, 0.25, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0], [0.25, 0.25, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0, 0.25, 0.25, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}